<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FastDeploy: fastdeploy::RuntimeOption Struct Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">FastDeploy
   &#160;<span id="projectnumber">latest</span>
   </div>
   <div id="projectbrief">Fast &amp; Easy to Deploy!</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacefastdeploy.html">fastdeploy</a></li><li class="navelem"><a class="el" href="structfastdeploy_1_1RuntimeOption.html">RuntimeOption</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="structfastdeploy_1_1RuntimeOption-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">fastdeploy::RuntimeOption Struct Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Option object used when create a new <a class="el" href="structfastdeploy_1_1Runtime.html" title="Runtime object used to inference the loaded model on different devices. ">Runtime</a> object.  
 <a href="structfastdeploy_1_1RuntimeOption.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="runtime__option_8h_source.html">runtime_option.h</a>&gt;</code></p>
<div class="dynheader">
Collaboration diagram for fastdeploy::RuntimeOption:</div>
<div class="dyncontent">
<div class="center"><img src="structfastdeploy_1_1RuntimeOption__coll__graph.png" border="0" usemap="#fastdeploy_1_1RuntimeOption_coll__map" alt="Collaboration graph"/></div>
<map name="fastdeploy_1_1RuntimeOption_coll__map" id="fastdeploy_1_1RuntimeOption_coll__map">
<area shape="rect" id="node2" href="structfastdeploy_1_1OrtBackendOption.html" title="Option object to configure ONNX Runtime backend. " alt="" coords="342,5,559,32"/>
<area shape="rect" id="node3" href="structfastdeploy_1_1PaddleBackendOption.html" title="Option object to configure Paddle Inference backend. " alt="" coords="352,57,549,98"/>
<area shape="rect" id="node4" href="structfastdeploy_1_1IpuOption.html" title="Option object to configure GraphCore IPU. " alt="" coords="31,35,193,61"/>
<area shape="rect" id="node5" href="structfastdeploy_1_1TrtBackendOption.html" title="Option object to configure TensorRT backend. " alt="" coords="5,104,219,131"/>
<area shape="rect" id="node6" href="structfastdeploy_1_1LiteBackendOption.html" title="Option object to configure Paddle Lite backend. " alt="" coords="340,185,561,212"/>
<area shape="rect" id="node7" href="structfastdeploy_1_1OpenVINOBackendOption.html" title="Option object to configure OpenVINO backend. " alt="" coords="340,237,561,278"/>
<area shape="rect" id="node8" href="structfastdeploy_1_1PorosBackendOption.html" title="Option object to configure Poros backend. " alt="" coords="355,302,546,343"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a0e1b51ddc437d5369e33205cb4c0ac0f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a0e1b51ddc437d5369e33205cb4c0ac0f">SetModelPath</a> (const std::string &amp;model_path, const std::string &amp;params_path=&quot;&quot;, const <a class="el" href="namespacefastdeploy.html#a3493c6d2ca1a50ca5506a0e195d0d723">ModelFormat</a> &amp;format=ModelFormat::PADDLE)</td></tr>
<tr class="memdesc:a0e1b51ddc437d5369e33205cb4c0ac0f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set path of model file and parameter file.  <a href="#a0e1b51ddc437d5369e33205cb4c0ac0f">More...</a><br /></td></tr>
<tr class="separator:a0e1b51ddc437d5369e33205cb4c0ac0f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a183aad68fda88d0618d3219e289e85d8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a183aad68fda88d0618d3219e289e85d8">SetModelBuffer</a> (const std::string &amp;model_buffer, const std::string &amp;params_buffer=&quot;&quot;, const <a class="el" href="namespacefastdeploy.html#a3493c6d2ca1a50ca5506a0e195d0d723">ModelFormat</a> &amp;format=ModelFormat::PADDLE)</td></tr>
<tr class="memdesc:a183aad68fda88d0618d3219e289e85d8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specify the memory buffer of model and parameter. Used when model and params are loaded directly from memory.  <a href="#a183aad68fda88d0618d3219e289e85d8">More...</a><br /></td></tr>
<tr class="separator:a183aad68fda88d0618d3219e289e85d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1176d5175cf5b29bd932fe548da33f45"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a1176d5175cf5b29bd932fe548da33f45">SetEncryptionKey</a> (const std::string &amp;encryption_key)</td></tr>
<tr class="memdesc:a1176d5175cf5b29bd932fe548da33f45"><td class="mdescLeft">&#160;</td><td class="mdescRight">When loading encrypted model, encryption_key is required to decrypte model.  <a href="#a1176d5175cf5b29bd932fe548da33f45">More...</a><br /></td></tr>
<tr class="separator:a1176d5175cf5b29bd932fe548da33f45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a98bd26740eaaaf8d24c378ee36977aed"><td class="memItemLeft" align="right" valign="top"><a id="a98bd26740eaaaf8d24c378ee36977aed"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a98bd26740eaaaf8d24c378ee36977aed">UseCpu</a> ()</td></tr>
<tr class="memdesc:a98bd26740eaaaf8d24c378ee36977aed"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use cpu to inference, the runtime will inference on CPU by default. <br /></td></tr>
<tr class="separator:a98bd26740eaaaf8d24c378ee36977aed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7a242e56140f3d2e4e8fd4c178580949"><td class="memItemLeft" align="right" valign="top"><a id="a7a242e56140f3d2e4e8fd4c178580949"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a7a242e56140f3d2e4e8fd4c178580949">UseGpu</a> (int gpu_id=0)</td></tr>
<tr class="memdesc:a7a242e56140f3d2e4e8fd4c178580949"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use Nvidia GPU to inference. <br /></td></tr>
<tr class="separator:a7a242e56140f3d2e4e8fd4c178580949"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad5403af7cdb416eb737028ff112d43c1"><td class="memItemLeft" align="right" valign="top"><a id="ad5403af7cdb416eb737028ff112d43c1"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#ad5403af7cdb416eb737028ff112d43c1">UseRKNPU2</a> (fastdeploy::rknpu2::CpuName rknpu2_name=fastdeploy::rknpu2::CpuName::RK356X, fastdeploy::rknpu2::CoreMask rknpu2_core=fastdeploy::rknpu2::CoreMask::RKNN_NPU_CORE_AUTO)</td></tr>
<tr class="memdesc:ad5403af7cdb416eb737028ff112d43c1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use RKNPU2 e.g RK3588/RK356X to inference. <br /></td></tr>
<tr class="separator:ad5403af7cdb416eb737028ff112d43c1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b132f7a7c01f7b96573494510090b39"><td class="memItemLeft" align="right" valign="top"><a id="a3b132f7a7c01f7b96573494510090b39"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a3b132f7a7c01f7b96573494510090b39">UseTimVX</a> ()</td></tr>
<tr class="memdesc:a3b132f7a7c01f7b96573494510090b39"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use TimVX e.g RV1126/A311D to inference. <br /></td></tr>
<tr class="separator:a3b132f7a7c01f7b96573494510090b39"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04fbda59e5acf73577ef38aa7b1732ce"><td class="memItemLeft" align="right" valign="top"><a id="a04fbda59e5acf73577ef38aa7b1732ce"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a04fbda59e5acf73577ef38aa7b1732ce">UseAscend</a> ()</td></tr>
<tr class="memdesc:a04fbda59e5acf73577ef38aa7b1732ce"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use Huawei Ascend to inference. <br /></td></tr>
<tr class="separator:a04fbda59e5acf73577ef38aa7b1732ce"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a08883d7a7fcb26097368bbbd1459cf17"><td class="memItemLeft" align="right" valign="top"><a id="a08883d7a7fcb26097368bbbd1459cf17"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a08883d7a7fcb26097368bbbd1459cf17">UseDirectML</a> ()</td></tr>
<tr class="memdesc:a08883d7a7fcb26097368bbbd1459cf17"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use onnxruntime DirectML to inference. <br /></td></tr>
<tr class="separator:a08883d7a7fcb26097368bbbd1459cf17"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6eddb9892477361c07472087a3602408"><td class="memItemLeft" align="right" valign="top"><a id="a6eddb9892477361c07472087a3602408"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a6eddb9892477361c07472087a3602408">UseSophgo</a> ()</td></tr>
<tr class="memdesc:a6eddb9892477361c07472087a3602408"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use Sophgo to inference. <br /></td></tr>
<tr class="separator:a6eddb9892477361c07472087a3602408"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6c93e9144900a46048167c2567c7195b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a6c93e9144900a46048167c2567c7195b">UseKunlunXin</a> (int kunlunxin_id=0, int l3_workspace_size=0xfffc00, bool locked=false, bool autotune=true, const std::string &amp;autotune_file=&quot;&quot;, const std::string &amp;precision=&quot;int16&quot;, bool adaptive_seqlen=false, bool enable_multi_stream=false)</td></tr>
<tr class="memdesc:a6c93e9144900a46048167c2567c7195b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Turn on KunlunXin XPU.  <a href="#a6c93e9144900a46048167c2567c7195b">More...</a><br /></td></tr>
<tr class="separator:a6c93e9144900a46048167c2567c7195b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49c808f148157e9e3f4e49cdf4be77cd"><td class="memItemLeft" align="right" valign="top"><a id="a49c808f148157e9e3f4e49cdf4be77cd"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a49c808f148157e9e3f4e49cdf4be77cd">UsePaddleInferBackend</a> ()</td></tr>
<tr class="memdesc:a49c808f148157e9e3f4e49cdf4be77cd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set Paddle Inference as inference backend, support CPU/GPU. <br /></td></tr>
<tr class="separator:a49c808f148157e9e3f4e49cdf4be77cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad64cc21325c28b00a7627be347b8078"><td class="memItemLeft" align="right" valign="top"><a id="aad64cc21325c28b00a7627be347b8078"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#aad64cc21325c28b00a7627be347b8078">UseOrtBackend</a> ()</td></tr>
<tr class="memdesc:aad64cc21325c28b00a7627be347b8078"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set ONNX <a class="el" href="structfastdeploy_1_1Runtime.html" title="Runtime object used to inference the loaded model on different devices. ">Runtime</a> as inference backend, support CPU/GPU. <br /></td></tr>
<tr class="separator:aad64cc21325c28b00a7627be347b8078"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affc01452e36e0bd703fa953eb97439f6"><td class="memItemLeft" align="right" valign="top"><a id="affc01452e36e0bd703fa953eb97439f6"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#affc01452e36e0bd703fa953eb97439f6">UseSophgoBackend</a> ()</td></tr>
<tr class="memdesc:affc01452e36e0bd703fa953eb97439f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set SOPHGO <a class="el" href="structfastdeploy_1_1Runtime.html" title="Runtime object used to inference the loaded model on different devices. ">Runtime</a> as inference backend, support SOPHGO. <br /></td></tr>
<tr class="separator:affc01452e36e0bd703fa953eb97439f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9f08901e06aabd15303cf516eb4587cc"><td class="memItemLeft" align="right" valign="top"><a id="a9f08901e06aabd15303cf516eb4587cc"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a9f08901e06aabd15303cf516eb4587cc">UseTrtBackend</a> ()</td></tr>
<tr class="memdesc:a9f08901e06aabd15303cf516eb4587cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set TensorRT as inference backend, only support GPU. <br /></td></tr>
<tr class="separator:a9f08901e06aabd15303cf516eb4587cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04ade14294b97e3b71e961a888e5fcaa"><td class="memItemLeft" align="right" valign="top"><a id="a04ade14294b97e3b71e961a888e5fcaa"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a04ade14294b97e3b71e961a888e5fcaa">UsePorosBackend</a> ()</td></tr>
<tr class="memdesc:a04ade14294b97e3b71e961a888e5fcaa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set Poros backend as inference backend, support CPU/GPU. <br /></td></tr>
<tr class="separator:a04ade14294b97e3b71e961a888e5fcaa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7992b766e32988373e85ca859f0ea533"><td class="memItemLeft" align="right" valign="top"><a id="a7992b766e32988373e85ca859f0ea533"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a7992b766e32988373e85ca859f0ea533">UseOpenVINOBackend</a> ()</td></tr>
<tr class="memdesc:a7992b766e32988373e85ca859f0ea533"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set OpenVINO as inference backend, only support CPU. <br /></td></tr>
<tr class="separator:a7992b766e32988373e85ca859f0ea533"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a7365011acb7b10c1d67d0e5ee1e765"><td class="memItemLeft" align="right" valign="top"><a id="a1a7365011acb7b10c1d67d0e5ee1e765"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a1a7365011acb7b10c1d67d0e5ee1e765">UsePaddleLiteBackend</a> ()</td></tr>
<tr class="memdesc:a1a7365011acb7b10c1d67d0e5ee1e765"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set Paddle Lite as inference backend, only support arm cpu. <br /></td></tr>
<tr class="separator:a1a7365011acb7b10c1d67d0e5ee1e765"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80b7a4e789ff431b79c97e6857a55e60"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a80b7a4e789ff431b79c97e6857a55e60">UseIpu</a> (int device_num=1, int micro_batch_size=1, bool enable_pipelining=false, int batches_per_step=1)</td></tr>
<tr class="separator:a80b7a4e789ff431b79c97e6857a55e60"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:aa9a2f18e1bef5737057be860fa4fd22b"><td class="memItemLeft" align="right" valign="top"><a id="aa9a2f18e1bef5737057be860fa4fd22b"></a>
<a class="el" href="structfastdeploy_1_1OrtBackendOption.html">OrtBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#aa9a2f18e1bef5737057be860fa4fd22b">ort_option</a></td></tr>
<tr class="memdesc:aa9a2f18e1bef5737057be860fa4fd22b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure ONNX <a class="el" href="structfastdeploy_1_1Runtime.html" title="Runtime object used to inference the loaded model on different devices. ">Runtime</a> backend. <br /></td></tr>
<tr class="separator:aa9a2f18e1bef5737057be860fa4fd22b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af918ebc77bbff0a193a1e9bdbce96796"><td class="memItemLeft" align="right" valign="top"><a id="af918ebc77bbff0a193a1e9bdbce96796"></a>
<a class="el" href="structfastdeploy_1_1TrtBackendOption.html">TrtBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#af918ebc77bbff0a193a1e9bdbce96796">trt_option</a></td></tr>
<tr class="memdesc:af918ebc77bbff0a193a1e9bdbce96796"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure TensorRT backend. <br /></td></tr>
<tr class="separator:af918ebc77bbff0a193a1e9bdbce96796"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a175f20ebcf388ca553b24e1a5632154e"><td class="memItemLeft" align="right" valign="top"><a id="a175f20ebcf388ca553b24e1a5632154e"></a>
<a class="el" href="structfastdeploy_1_1PaddleBackendOption.html">PaddleBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a175f20ebcf388ca553b24e1a5632154e">paddle_infer_option</a></td></tr>
<tr class="memdesc:a175f20ebcf388ca553b24e1a5632154e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure Paddle Inference backend. <br /></td></tr>
<tr class="separator:a175f20ebcf388ca553b24e1a5632154e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad33fdf56d984518d2945a024db058913"><td class="memItemLeft" align="right" valign="top"><a id="ad33fdf56d984518d2945a024db058913"></a>
<a class="el" href="structfastdeploy_1_1PorosBackendOption.html">PorosBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#ad33fdf56d984518d2945a024db058913">poros_option</a></td></tr>
<tr class="memdesc:ad33fdf56d984518d2945a024db058913"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure Poros backend. <br /></td></tr>
<tr class="separator:ad33fdf56d984518d2945a024db058913"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abb0adac51a9edd15a115090c4b671282"><td class="memItemLeft" align="right" valign="top"><a id="abb0adac51a9edd15a115090c4b671282"></a>
<a class="el" href="structfastdeploy_1_1OpenVINOBackendOption.html">OpenVINOBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#abb0adac51a9edd15a115090c4b671282">openvino_option</a></td></tr>
<tr class="memdesc:abb0adac51a9edd15a115090c4b671282"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure OpenVINO backend. <br /></td></tr>
<tr class="separator:abb0adac51a9edd15a115090c4b671282"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b08922255dacbb1341a5a1542566008"><td class="memItemLeft" align="right" valign="top"><a id="a9b08922255dacbb1341a5a1542566008"></a>
<a class="el" href="structfastdeploy_1_1LiteBackendOption.html">LiteBackendOption</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a9b08922255dacbb1341a5a1542566008">paddle_lite_option</a></td></tr>
<tr class="memdesc:a9b08922255dacbb1341a5a1542566008"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure Paddle Lite backend. <br /></td></tr>
<tr class="separator:a9b08922255dacbb1341a5a1542566008"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90959f5aeb801a81a94b984aaee85d97"><td class="memItemLeft" align="right" valign="top"><a id="a90959f5aeb801a81a94b984aaee85d97"></a>
RKNPU2BackendOption&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfastdeploy_1_1RuntimeOption.html#a90959f5aeb801a81a94b984aaee85d97">rknpu2_option</a></td></tr>
<tr class="memdesc:a90959f5aeb801a81a94b984aaee85d97"><td class="mdescLeft">&#160;</td><td class="mdescRight">Option to configure RKNPU2 backend. <br /></td></tr>
<tr class="separator:a90959f5aeb801a81a94b984aaee85d97"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Option object used when create a new <a class="el" href="structfastdeploy_1_1Runtime.html" title="Runtime object used to inference the loaded model on different devices. ">Runtime</a> object. </p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="a1176d5175cf5b29bd932fe548da33f45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1176d5175cf5b29bd932fe548da33f45">&#9670;&nbsp;</a></span>SetEncryptionKey()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fastdeploy::RuntimeOption::SetEncryptionKey </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>encryption_key</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>When loading encrypted model, encryption_key is required to decrypte model. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">encryption_key</td><td>The key for decrypting model </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a183aad68fda88d0618d3219e289e85d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a183aad68fda88d0618d3219e289e85d8">&#9670;&nbsp;</a></span>SetModelBuffer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fastdeploy::RuntimeOption::SetModelBuffer </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>model_buffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>params_buffer</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacefastdeploy.html#a3493c6d2ca1a50ca5506a0e195d0d723">ModelFormat</a> &amp;&#160;</td>
          <td class="paramname"><em>format</em> = <code>ModelFormat::PADDLE</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Specify the memory buffer of model and parameter. Used when model and params are loaded directly from memory. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">model_buffer</td><td>The string of model memory buffer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">params_buffer</td><td>The string of parameters memory buffer </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">format</td><td>Format of the loaded model </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a0e1b51ddc437d5369e33205cb4c0ac0f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0e1b51ddc437d5369e33205cb4c0ac0f">&#9670;&nbsp;</a></span>SetModelPath()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fastdeploy::RuntimeOption::SetModelPath </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>model_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>params_path</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacefastdeploy.html#a3493c6d2ca1a50ca5506a0e195d0d723">ModelFormat</a> &amp;&#160;</td>
          <td class="paramname"><em>format</em> = <code>ModelFormat::PADDLE</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Set path of model file and parameter file. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">model_path</td><td>Path of model file, e.g ResNet50/model.pdmodel for Paddle format model / ResNet50/model.onnx for ONNX format model </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">params_path</td><td>Path of parameter file, this only used when the model format is Paddle, e.g Resnet50/model.pdiparams </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">format</td><td>Format of the loaded model </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a80b7a4e789ff431b79c97e6857a55e60"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80b7a4e789ff431b79c97e6857a55e60">&#9670;&nbsp;</a></span>UseIpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fastdeploy::RuntimeOption::UseIpu </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>device_num</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>micro_batch_size</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_pipelining</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batches_per_step</em> = <code>1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Graphcore IPU to inference.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">device_num</td><td>the number of IPUs. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">micro_batch_size</td><td>the batch size in the graph, only work when graph has no batch shape info. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">enable_pipelining</td><td>enable pipelining. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">batches_per_step</td><td>the number of batches per run in pipelining. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a6c93e9144900a46048167c2567c7195b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6c93e9144900a46048167c2567c7195b">&#9670;&nbsp;</a></span>UseKunlunXin()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fastdeploy::RuntimeOption::UseKunlunXin </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>kunlunxin_id</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>l3_workspace_size</em> = <code>0xfffc00</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>locked</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>autotune</em> = <code>true</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>autotune_file</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>precision</em> = <code>&quot;int16&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>adaptive_seqlen</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_multi_stream</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Turn on KunlunXin XPU. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">kunlunxin_id</td><td>the KunlunXin XPU card to use (default is 0). </td></tr>
    <tr><td class="paramname">l3_workspace_size</td><td>The size of the video memory allocated by the l3 cache, the maximum is 16M. </td></tr>
    <tr><td class="paramname">locked</td><td>Whether the allocated L3 cache can be locked. If false, it means that the L3 cache is not locked, and the allocated L3 cache can be shared by multiple models, and multiple models sharing the L3 cache will be executed sequentially on the card. </td></tr>
    <tr><td class="paramname">autotune</td><td>Whether to autotune the conv operator in the model. If true, when the conv operator of a certain dimension is executed for the first time, it will automatically search for a better algorithm to improve the performance of subsequent conv operators of the same dimension. </td></tr>
    <tr><td class="paramname">autotune_file</td><td>Specify the path of the autotune file. If autotune_file is specified, the algorithm specified in the file will be used and autotune will not be performed again. </td></tr>
    <tr><td class="paramname">precision</td><td>Calculation accuracy of multi_encoder </td></tr>
    <tr><td class="paramname">adaptive_seqlen</td><td>Is the input of multi_encoder variable length </td></tr>
    <tr><td class="paramname">enable_multi_stream</td><td>Whether to enable the multi stream of KunlunXin XPU. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<hr/>The documentation for this struct was generated from the following files:<ul>
<li>/fastdeploy/my_work/FastDeploy/fastdeploy/runtime/<a class="el" href="runtime__option_8h_source.html">runtime_option.h</a></li>
<li>/fastdeploy/my_work/FastDeploy/fastdeploy/runtime/runtime_option.cc</li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
